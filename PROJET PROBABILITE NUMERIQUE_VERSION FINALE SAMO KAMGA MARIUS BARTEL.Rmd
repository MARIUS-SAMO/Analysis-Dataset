---
title: "PROJET PROBABILITE NUMERIQUES_version finale"
author: "SAMO KAMGA MARIUS BARTEL"
date: "03/11/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# SIMULATION DES LOIS USUELLES 

## SIMULATION D'UNE LOI EXPONENTIELLE

Le code suivant permet de simuler une loi exponentielle de paramètre $\lambda>0$ en utilisant un loi uniforme sur [0,1], et en inversant sa fonction de répartition.le procédé est décrit comme suit:
 $X \sim \mathcal{E}(\lambda)$
On note $F_\lambda$ la fonction de répartition de X. On a $F_\lambda(x)=\mathbb{P}[\mathcal{E}(\lambda)\leq x] = \int_0^x \lambda exp(-\lambda y)dy = 1-exp(-\lambda x)$

Soit $u\in[0,1]$, on a $u=F_\lambda(x) \Leftrightarrow u= 1-\exp(-\lambda x) \Leftrightarrow x= -\frac{1}{\lambda}\log(1-u)$ 
d'où on obtient : si $U \sim \mathcal{U}_{[0.1]}$ alors  $-\frac{1}{\lambda}\log(U) \sim \mathcal{E}(\lambda)$

 
```{r}


#U<-runif(n,0,1)  génère n réalisation de la loi uniforme sur [0,1]

simulation_exp<-function(lambda,n)   # Renvoie un vecteur de n simulations d'une loi exponentielle de paramètre lambda
{
  return(-log(runif(n,0,1))/lambda)
}
simulation_exp(2,20)
```
## SIMULATION D'UNE LOI DE POISSON

Le code suivant simule une loi de poisson de paramètre $\lambda>0$ à partir des réalisations d'une loi exponentielle de paramètre $\lambda>0$ , il est basé sur ce principe:
pour tout $n$ entier naturel, on a $\mathbb{P}(\mathcal{P}(\lambda)=n) = \mathbb{P} \left( \sum_{i=1}^n E_i \leq 1 \leq \sum_{i=1}^{n+1} E_i \right )$ avec $E_i$ variable aléatoire de loi $\mathcal{E}(\lambda)$ pour tout $i$. 

```{r}
simu_n_poisson<-function(n,lambda){
   simulation_poisson<-function(lambda)      # génère une réalisation de la loi de poisson de paramètre lambda
   {
       sum<-simulation_exp(lambda,1)               
       comptage<-0
       while(sum<=1)
         {
            sum<-sum+simulation_exp(lambda,1)
            comptage<-comptage +1
          }
       return(comptage)
 
    }
    return(replicate(n,simulation_poisson(lambda)))   # retourne un  vecteur de n réalisation de la loi de Poisson de paramètre lambda 
    
 }                                                       
simu_n_poisson(10,3)
```

## SIMULATION D'UNE LOI NORMALE CENTRE REDUITE

Pour simuler cette loi nous allons utiliser la Méthode de Box-Muller:
Soit $U_1$ et $U_2$ deux variables aléatoires indépendantes uniformément distribuées dans $]0,1]$ 
Alors $Z_0 := \sqrt{-2\ln{U_1}}\cos(2\pi U_2)$ et $Z_1 := \sqrt{-2\ln{U_1}}\sin(2\pi U_2)$ sont des variables aléatoires indépendantes suivant une loi normale centrée réduite.puis on retient Z_0 ( dans le code il est renommée Échantillon_Norm) ceci car l'on a besoin que d'un seul échantillon.

```{r}
simulation_normale<-function(n)
{
U1<-runif(n,0,1)
U2<-runif(n,0,1)
Echantillon_Norm<-sqrt(-2*log(U1))*cos(2*pi*U2)  # n échantillons de la loi normale
Echantillon_Norm
}
simulation_normale(10)
```

## SIMULATION DE LA LOI DE CAUCHY 

Le code suivant permet de simuler une loi de Cauchy en utilisant un loi uniforme sur [0,1], et  en inversant sa fonction de répartition . le procédé utilisé est décrit comme suit:
Soit $F_c(x)$ la fonction de réparition de la loi de Cauchy, on a $F_c(x) = \int_{-\infty}^x \frac{1}{\pi(1+x^2)}dx = \frac{1}{\pi}\left( \arctan(x) +\frac{\pi}{2}  \right)$ ,soit  $u  \in [0,1]$ ,$F_c^{-1}(u)=\tan\left( \pi \left(  u-\frac{1}{2} \right) \right)$

```{r}
set.seed(121)
simu_cauchy<- function(n)
{
  M<-runif(n)
  cauchy<-tan(pi*(M-0.5))         # simulation par inversion de la fonction de répartition 
}
replicate(n=10,mean(simu_cauchy(1000)),simplify = F) # calcul de la moyenne de l'échantillon dix fois consécutifs

# on remarque la moyenne est a chaque fois différente car l'hypothèse d'intégrabilité n'est pas vérifier pour la loi de Cauchy et par conséquent la loi des grands nombres ne l'est pas non plus.

```






# SIMULATION DE L'INTERVALLE DE CONFIANCE ET VERIFICATION DE LA LOI DES GRANDS NOMBRES

##  CAS DE LA LOI EXPONENTIELLE

Principe :
Nous allons maintenant vérifier que la probabilité que le paramètre que l'on cherche à estimer (en l'occurence $\frac{1}{\lambda}$) se trouve bien dans un intervalle centré autour de la moyenne empirique $\overline{X_n}$ de notre simulation.
Pour cela nous allons appliquer le Théorème Centrale Limite ainsi que le Lemme de Slutsky:
$$ \frac{\sqrt{n}}{\hat{\sigma_n}}\left( \overline{X_n} - \frac{1}{\lambda} \right) \underset{n\to +\infty}{\overset{\mathcal{L}}{\longrightarrow}} \mathcal{N}(0,1) $$
Avec $\hat{\sigma_n}$ l'estimateur sans biais de l'écart type, c'est-à-dire tel que
$\hat{\sigma_n}^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \overline{X_n})^2$

On va donc construire un intervalle de confiance au seuil $\alpha$, avec $\alpha = 0.95$ et donc le quantile d'ordre $\alpha$ de la loi $\mathcal{N}(0,1)$ est $1.96$

La convergence en loi donne que $\mathbb{P} \left [\frac{\sqrt{n}}{\hat{\sigma_n}}\left( \overline{X_n} - \frac{1}{\lambda} \right) \in [-1.96,1.96] \right  ] \simeq  \mathbb{P} [\mathcal{N}(0,1) \in [-1.96,1.96]] =\alpha$.

D'où $\mathbb{P} \left [\frac{1}{\lambda} \in \left [ \overline{X_n} - \frac{1.96 \times \hat{\sigma _n}}{\sqrt{n}} , \overline{X_n} + \frac{1.96 \times \hat{\sigma _n}}{\sqrt{n}}  \right ] \right ] \simeq \alpha$.

par ailleurs nous illustrerons aussi le fait que : $\overline{X_n} \underset{\mathbb{P}-p.s}{\overset{n}{\longrightarrow}} \mathbb{E}[X]$

le code suivant permet de vérifier( visualiser graphiquement) la loi des grands nombres et de tracer l'intervalle de confiance a 95% pour une distribution exponentielle
de paramètre $\alpha=s1$ 
```{r}
set.seed(121)
simu_ic<- function(n,s1){
M<-simulation_exp(s1,n)   
X<-cumsum(M)                                        # somme cumulé de M
V<-cumsum(M^2)                                      # somme cumulé de M^2
A<-1:n
X_empirique<-X/A                                    # Moyenne empirique
R<-A/max(A,1)                                       # facteur pour calculé l'estimateur sans biais de la variance                                                      
V_empirique<-(V/max(A-1,1))-((X_empirique)^2)*R     # variance empirique
ampli_ic<-(1.96*sqrt(V_empirique))/sqrt(A)          # amplitude de l'intervalle de confiance
ic_inf<-X_empirique-ampli_ic                        # borne inf de l'intervalle de confiance
ic_sup<- X_empirique+ampli_ic                       # borne sup de l'intervalle de confiance
plot(X_empirique,type = "l",col= " blue ")          # représentation graphique la moyenne empirique en bleu
lines(ic_inf,type = "l",col= " red ")               # représentation graphique la borne inf de l'ic  en rouge
lines(ic_sup,type = "l",col=" yellow")              # représentation graphique la borne sup de l'ic en jaune
cat (" la moyenne empirique est égal a : ",X_empirique[length(X_empirique)]," ce qui est très proche de la moyenne théorique qui vaut : ",1/s1," ainsi la loi des grands nombres est vérifiée",sep="")
}
simu_ic(10000,0.5)                           

# la fonction permet aussi de changer le nombre d'observation ou le paramètre de la loi mais les résultats estimés reste proche des résultats théoriques ( pour n assez grand)



```



la description pour les deux autres lois a venir est la même que ci dessus(même algorithme)  donc pour éviter de surcharger le code nous l’omettrons

#  CAS DE LA LOI DE POISSON

Le principe est le même que utilisé précédemment 
le code suivant permet de vérifier( visualiser) la loi des grands nombres et de tracer l'intervalle de confiance a 95% pour une distribution de poisson
de paramètre s2 .
```{r}
set.seed(121)
simu_ic<- function(n,s2){
M<-simu_n_poisson(n,s2)
X<-cumsum(M)
V<-cumsum(M^2)
A<-1:n
X_empirique<-X/A
R<-A/max(A,1)
V_empirique<-(V/max(A-1,1))-((X_empirique)^2)*R
ampli_ic<-(1.96*sqrt(V_empirique))/sqrt(A)
ic_inf<-X_empirique-ampli_ic
ic_sup<- X_empirique+ampli_ic
plot(X_empirique,type = "l",col= " blue ")
lines(ic_inf,type = "l",col= " red ")
lines(ic_sup,type = "l",col=" yellow")
cat (" la moyenne empirique est égal a : ",X_empirique[length(X_empirique)]," ce qui est très proche de la moyenne théorique qui vaut : ",s2," ,ainsi la loi des grands nombres est vérifié",sep="")
}
simu_ic(10000,3)

```


# CAS DE LA LOI NORMALE CENTRE REDUITE
Même principe que pour les deux autres 
le code suivant permet de vérifier la loi des grands nombres et de tracer l'intervalle de confiance a 95% pour une distribution normale centré réduite

```{r}
set.seed(121)
simu_ic<- function(n){
M<-simulation_normale(n)
X<-cumsum(M)
V<-cumsum(M^2)
A<-1:n
X_empirique<-X/A
R<-A/max(A,1)
V_empirique<-(V/max(A-1,1))-((X_empirique)^2)*R
ampli_ic<-(1.96*sqrt(V_empirique))/sqrt(A)
ic_inf<-X_empirique-ampli_ic
ic_sup<- X_empirique+ampli_ic
plot(X_empirique,type = "l",col= " blue ")
lines(ic_inf,type = "l",col= " red ")
lines(ic_sup,type = "l",col=" yellow")
cat (" la moyenne empirique est égal a : ",X_empirique[length(X_empirique)]," ce qui est très proche de la moyenne théorique qui vaut : ",0," ,ansi la loi des grands nombres est vérifiée",sep="")
}
simu_ic(10000)


```

## VERIFICATION DU THEOREME CENTRALE LIMITE

principe :

On va calculer $Zn:=\frac{\sqrt{n}}{\hat{\sigma_n}}\left( \overline{X_n} - \mathbb{E}[X] \right)$ un grand nombre de fois ( rows fois) avec $n=100$ puis on va représenter cela sur un histogramme et comparer avec la distribution d'une loi normale $\mathcal{N}(0,1)$.

# CAS DE LA DISTRIBUTION DE POISSON
Le code suivant permet de vérifier le TCL pour une distribution de poisson de paramètre lambda égale a 5
```{r}
set.seed(121)
lambda_poisson<-5
n<-100
rows<-1000
sim<-simu_n_poisson(n*rows,lambda_poisson)          #  génère un vecteur de distribution de poisson de taille n*rows
m<-matrix(sim,rows)                                 # matrice de rows échantillons avec par ligne n résultat aléatoire de la distribution de poisson
sample.mean<-rowMeans(m)                            # vecteur contenant la moyenne de chaque ligne
R<-(sample.mean-lambda_poisson)*(sqrt(n)/sqrt(lambda_poisson))      # normalisation
hist(R,breaks = 30,prob=TRUE)                       # affichage de l'histogramme 
a<-pretty(-4:4,40)      
q<-dnorm(a)
lines(a,q,type = "l",col="blue")                   # ajout de la densité de la loi normale centré réduite

#  on constate bien ainsi que le théorème central limite est vérifié
```

# CAS DE LA DISTRIBUTION EXPONENTIELLE
Le code suivant permet de vérifier le TCL pour une distribution exponentielle de paramètre lambda égale a 1
```{r}
set.seed(121)
lambda_expo<-1
n<-1000
rows<-10000
sim<-simulation_exp(lambda_expo,n*rows)
m<-matrix(sim,rows)
sample.mean<-rowMeans(m)
R<-(sample.mean-lambda_expo)*(sqrt(n)/sqrt(lambda_expo))
hist(R,breaks = 30,prob=TRUE)
a<-pretty(-4:4,30)
q<-dnorm(a)
lines(a,q,type = "l",col="blue")
```

# CAS DE LA DISTRIBUTION NORMALE CENTREE REDUITE
Le code suivant permet de vérifier le TCL pour une distribution normale centré réduite
```{r}

set.seed(121)
n<-1000
rows<-10000
sim<-simulation_normale(n*rows)
m<-matrix(sim,rows)
sample.mean<-rowMeans(m)
R<-(sample.mean)*sqrt(n)
hist(R,breaks = 30,prob=TRUE)
a<-pretty(-4:4,30)
q<-dnorm(a)
lines(a,q,type = "l",col="blue")

```


CONCLUSION : on observe graphiquement que plus la taille de notre échantillon est grande plus sa distribution se rapproche de cella de la loi normale $\mathcal{N}(0,1)$




# PARTIE 6 : MONTE CARLO
Dans cette partie nous allons reformuler des intégrales en terme d'espérance pour pouvoir les estimer à l'aide d'une méthode de Monte Carlo, les codes serons écrit aussi dans le but de trouver a chaque fois un intervalle de confiance a 95% , en fait nous ferons à chaque fois une estimation empirique des espérances.

## CAS DE I(1)
Estimation de l'intégrale $\int_0^1 4\sqrt{1-x^2}dx := I_1$.

On note $f_1$ la fonction $x \mapsto 4\sqrt{1-x^2}$ , $X$ la variable aléatoire de loi uniforme sur $[0,1]$ et $g_X$ une densité de $X$ : $g_X(x) = \mathbf{1}_{[0,1]}$. 
On a $\mathbb{E}(f_1(X))=\int_{\mathbb{R}}f_1(x)g(x)dx = I_1$, donc on peut approcher cette espérance en calculant un estimateur de $I_1$ à partir d'un échantillon $(x_1,x_2,\cdots,x_n)$ de la loi de $X$ sur $[0,1]$.

La loi forte des grands nombres nous suggère de prendre l'estimateur de la moyenne empirique $\hat{I_n}:=\frac{1}{n}\sum_{i=1}^n f_1(x_i)$ qui est un estimateur sans biais de l'espérance que l'on veut calculer.
Ainsi, il nous suffit de calculer $\hat{I_n}$ pour n grand afin d'avoir une valeur approché de $I_1$. 


```{r}
set.seed(121)
simu_ic1<- function(n){
M<-runif(n,min =0,max=1)     # simulation uniforme sur [0,1]
M<-4*sqrt(1-(M)^2)           # simulation par inversion de la fonction de répartition
X<-cumsum(M)
V<-cumsum(M^2)
A<-1:n
X_empirique<-X/A
X_empirique[length(X_empirique)]
R<-A/max(A,1)
V_empirique<-(V/max(A-1,1))-((X_empirique)^2)*R
ampli_ic<-(1.96*sqrt(V_empirique))/sqrt(A)
ic_inf<-X_empirique-ampli_ic
ic_inf[length(ic_inf)]
ic_sup<- X_empirique+ampli_ic
ic_sup[length(ic_sup)]
cat(" l'estimation empirique de I(1) est : ",X_empirique[length(X_empirique)]," et l'intervalle de confiance a 95% associé est : ","[",ic_inf[length(ic_inf)],",",ic_sup[length(ic_sup)],"]",sep = "")
}
simu_ic1(1000000)


```

# CAS DE I(2)

Estimation de l'intégrale $\int_{[-1,1]^2} \mathbf{1}_{x^2+y^2\leq 1}dxdy := I_2$. 

On note $f_2$ la fonction $(x,y) \mapsto \mathbf{1}_{x^2+y^2\leq 1} \times 4$ , $X$ et $Y$ deux variables aléatoires indépendantes de loi uniforme sur $[-1,1]$ et $g_{(X,Y)}$ la densité du couple $(X,Y)$ : $g_{(X,Y)}(x,y) =\mathbf{1}_{[-1,1]^2}(x,y)\times \frac{1}{4}$. 

On a $\mathbb{E}(f_2(X,Y))=\int_{\mathbb{R}^2}f_2(x,y)g(x,y)dxdy = I_2$, ainsi, comme précédemment, on calcule $\hat{I_n}:=\frac{1}{n}\sum_{i=1}^n f(x_i,y_i)$ à partir d'un échantillon $(x_1,x_2,\cdots,x_n)$ de la loi de $X$ et d'un échantillon $(y_1,y_2,\cdots,y_n)$ de la loi de $Y$.
```{r}
set.seed(121)
simu_ic_2<- function(n){
x<-runif(n,min=-1,max = 1)      # simulation uniforme sur [-1,1]
y<-runif(n,min = -1,max=1)
z<-x^2+y^2                      # distance des points a l'origine
z<-z<=1                         # recherche des points se trouvant dans le disque
X<-cumsum(z)
V<-cumsum(z^2)
A<-1:n
X_empirique<-X/A
a<-4*(X_empirique[length(X_empirique)])
R<-A/max(A,1)
V_empirique<-(V/max(A-1,1))-((X_empirique)^2)*R
ampli_ic<-(1.96*sqrt(V_empirique))/sqrt(A)
ic_inf<-X_empirique-ampli_ic
ic_inf[length(ic_inf)]
ic_sup<- X_empirique+ampli_ic
ic_sup[length(ic_sup)]
cat(" l'estimation empirique de I(2) est : ",a," et l'intervalle de confiance a 95% associé est : ","[",4*ic_inf[length(ic_inf)],",",4*ic_sup[length(ic_sup)],"]",sep = "")
}
simu_ic_2(100000)


```
# CAS DE I(3)
Estimation de l'intégrale $\int_{[-1,1]^3} \mathbf{1}_{x^2+y^2+z^2\leq 1}dxdydz := I_3$.

On note $f_3$ la fonction $(x,y,z) \mapsto \mathbf{1}_{x^2+y^2+z^2\leq 1} \times 8$ , $X$, $Y$ et $Z$ trois variables aléatoires indépendantes de loi uniforme sur $[-1,1]$ et $g_{(X,Y,Z)}$ la densité du triplet $(X,Y,Z)$ : $g_{(X,Y,Z)}(x,y,z) =\mathbf{1}_{[-1,1]^3}(x,y,z)\times \frac{1}{8}$.

On a $\mathbb{E}(f_3(X,Y,Z))=\int_{\mathbb{R}^3}f_3(x,y,z)g(x,y,z)dxdydz = I_3$, d'où on calcule $\hat{I_n}:=\frac{1}{n}\sum_{i=1}^n f(x_i,y_i,z_i)$ à partir d'un échantillon $(x_1,x_2,\cdots,x_n)$ de la loi de $X$ ,d'un échantillon $(y_1,y_2,\cdots,y_n)$ de la loi de $Y$ et d'un échantillon $(z_1,z_2,\cdots,z_n)$ de la loi de $Z$. 

```{r}
set.seed(121)
simu_ic_2<- function(n){
x<-runif(n,min=-1,max = 1)
y<-runif(n,min = -1,max=1)           # simulation uniforme sur [-1,1]
t<-runif(n,min = -1,max = 1)
z<-x^2+y^2+t^2                       # distance des points a l'origine
z<-z<=1                              # recherche des points se trouvant dans la boule
X<-cumsum(z)
V<-cumsum(z^2)
A<-1:n
X_empirique<-X/A
a<-8*(X_empirique[length(X_empirique)])
R<-A/max(A,1)
V_empirique<-(V/max(A-1,1))-((X_empirique)^2)*R
ampli_ic<-(1.96*sqrt(V_empirique))/sqrt(A)
ic_inf<-X_empirique-ampli_ic
ic_inf[length(ic_inf)]
ic_sup<- X_empirique+ampli_ic
ic_sup[length(ic_sup)]
cat(" l'estimation empirique de I(3) est : ",a," et l'intervalle de confiance a 95% associé est : ","[",8*ic_inf[length(ic_inf)],",",8*ic_sup[length(ic_sup)],"]",sep = "")
}
simu_ic_2(100000)


```

# METHODE DU REJET
Dans cette section il sera question de simuler des lois a partir de la méthode de rejet.

##  Loi uniforme sur la disque unité
Pour simuler une loi uniforme sur le disque unité, nous allons simuler deux lois uniformes sur $[-1,1]$ , nous aurons donc deux échantillons $(x_1,x_2,\cdots,x_n)$ et
$(y_1,y_2,\cdots,y_n)$, puis nous allons prendre les couples $(x_i,y_i)$ tel que le point $(x_i,y_i)$ soit dans le disque unité, c'est-à-dire les couples qui respectent l'inégalité $x_i^2 + y_i^2 \leq 1$.

```{r}
U1<-runif(10^4,-1,1)
U2<-runif(10^4,-1,1)
V1<-U1[U1^2 + U2^2 <1]   #On prend les abscisses des points qui respectent
V2<-U2[U1^2 + U2^2 <1]  #On prend les ordonnées des points qui respectent
plot(V1,V2)
```

## Simulation de lois avec une densité à support compact
On va simuler ici des variables aléatoires de densité à support compact.
Prenons le cas d'une variable aléatoire $X$ avec pour densité $f_X$ tel que $f_X(x)=\frac{\pi}{2}\sin(\pi x)\mathbf{1}_{x\in [0,1]}$

principe : On simule des lois uniformes sur [0,1]×[0,1] et l’on teste si la réalisation obtenue se trouve sous le graphe de la fonction $x \mapsto \sin(\pi x)$



```{r}
set.seed(121)
simu_rejet<- function(n)
{
  X<-rep(0,n)
  for (i in 1:n) 
{
 x<-runif(2)
 while (x[2]>sin(pi*x[1]))
{
   x<-runif(2)
 }
 X[i]<-x[1]
 
  }
hist(X,breaks = 100,probability = "TRUE")  
Y=seq(0,1,0.05)
lines(x = Y,(pi*sin(pi*Y))/2, type='l',col="blue")
}
simu_rejet(10000)

```

## simulation par rejet de la loi bêta de paramètre (4,2)

On peut faire de même avec une loi Bêta par exemple de paramètre $\alpha=4$ et $\beta = 2$, dont on rapelle la densité : $f(x)= 20x^3(1-x) \mathbf{1}_{[0,1]}$.
```{r}
U1<-runif(10^6,0,1)
U2<-runif(10^6,0,1)
V<-U1[U2<(U1^3*(1-U1))]


f<-function(X)
{
  return(20*(X^3*(1-X)))
}
m<-length(V)   # code pour afficher ma densité, en prenant abscisses et ordonnées 
X<-seq(0,1,length=m)
Fx<-f(X)    
hist(V,breaks = 100,probability = "TRUE") 
lines(x = X,Fx, type='l',col="blue")

```

# LOIS MELANGES
Dans cette section nous allons simuler des variables aléatoires dont la densité est une combinaison linéaire de densité connues
la sommes des coefficients dans cette combinaison linéaire doit valoir $1$ pour bien avoir une densité..

## Simulation loi exponentielle symétrique
La densité de la loi exponentielle symétrisée est : $f(x)=\frac12 \exp(-|x|)$. 
Une façon de la réaliser par mélange consiste à écrire $f(x)= \frac12 (f_{\mathcal{E}(1)}(x) + f_{\mathcal{E}(1)}(-x))$. Dans la pratique il suffit de tirer le signe en suivant une loi de Bernoulli de paramètre $\frac12$ et de multiplier le résultat obtenu par une réalisation de variable exponentielle de paramètre 1.
on le visualise a travers le code est le graphe
```{r}
simu_mel_expo<-function(M)
{
res<- rexp(M,1)*(-1+2*(runif(M,0,1)>(1/2)))
hist(res,breaks=100,prob="T")
a<-pretty(-5:5,30)
lines(x = a,y=dexp(a),col='blue')  # on affiche la densité de la loi exponentielle de paramètre 1 en bleu
lines(x = -a,y=dexp(a,rate=1),col='red') # on affiche la densité de sa symétrie en rouge
}
simu_mel_expo(10000)
```

## Simulation de lois mélange a partir de lois normales
Nous allons faire la même chose pour des lois normales:
on va simuler la variable aléatoire de densité $f(x)= \frac{1}{3}f_{\mathcal{N}(0,1)}(x) + \frac{2}{3}f_{\mathcal{N}(4,\frac{1}{4})}(x)$ où $f_{\mathcal{N}(m,\sigma ^2)} = \frac{1}{\sqrt{2 \pi} \sigma }\exp \left( -\frac{(x-m)^2}{2\sigma ^2} \right)$ désigne la densité de la loi normale de paramètres $(m,\sigma ^2)$ au point $x$. 
On le fait de la façon suivante puis on visualise

```{r}
set.seed(121)
simu_mel_normale<-function(n)
{
A<-runif(n)>1/3
res<-rep(0,n)
for(i in 1:n)
{
  res[i]<-rnorm(n=1,mean = 4*A[i],sd=1/(1+A[i]))
}
hist(res,breaks=100,prob='T')
}
simu_mel_normale(10000)


```

